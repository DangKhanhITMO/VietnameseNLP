{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDdR6EhtUL6f",
        "outputId": "b0ef9c02-26d6-48c3-fea9-aa0e146b80a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPoUCg2pV5ek",
        "outputId": "bd60a7a0-62da-47b7-b38d-f34e04e055f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vncorenlp\n",
        "!wget -qO vncorenlp.zip https://github.com/vncorenlp/VnCoreNLP/archive/master.zip\n",
        "!unzip -q vncorenlp.zip\n",
        "!mv VnCoreNLP-master VnCoreNLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFIc0S5_y2U6",
        "outputId": "0b11d850-eccb-40c6-c644-a5606d72decc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vncorenlp\n",
            "  Downloading vncorenlp-1.0.3.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vncorenlp) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vncorenlp) (2024.8.30)\n",
            "Building wheels for collected packages: vncorenlp\n",
            "  Building wheel for vncorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for vncorenlp: filename=vncorenlp-1.0.3-py3-none-any.whl size=2645932 sha256=4919556498fd212b3cc6f45e764d4e9079bc69788e3a697d0592456638134f0c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/d9/b3/41f6c6b1ab758561fd4aab55dc0480b9d7a131c6aaa573a3fa\n",
            "Successfully built vncorenlp\n",
            "Installing collected packages: vncorenlp\n",
            "Successfully installed vncorenlp-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kf5D5S5bUEhZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aG4-bgpVUEhc"
      },
      "outputs": [],
      "source": [
        "path_viSL_dict = '/content/drive/MyDrive/Data_paper_4/output.xlsx'\n",
        "path_data_test = '/content/drive/MyDrive/Data_paper_4/dataTest.txt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OVK_kQIDUEhd"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(path_viSL_dict)\n",
        "set_words = set(df.iloc[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcuiaUibUEhf",
        "outputId": "05d894b1-567e-4cf3-e711-34ea960b5dc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4332"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "len(set_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O5Rxk0nKUEhg"
      },
      "outputs": [],
      "source": [
        "dict_viSL = dict()\n",
        "for i, w in enumerate(set_words):\n",
        "  dict_viSL[w] = i+1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "import io\n",
        "\n",
        "# List of Unicode characters with accents\n",
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "# Corresponding characters without accents\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        "\n",
        "# Function to load the dictionary for mapping accented to unaccented characters\n",
        "def load_accent_dict():\n",
        "    accent_dict = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split('|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split('|')\n",
        "    for i in range(len(char1252)):\n",
        "        accent_dict[char1252[i]] = charutf8[i]\n",
        "    return accent_dict\n",
        "\n",
        "accent_dict = load_accent_dict()\n",
        "\n",
        "# Function to convert precomposed Unicode characters to decomposed form\n",
        "def convert_unicode(text):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: accent_dict[x.group()], text\n",
        "    )\n",
        "\n",
        "# Vietnamese vowel table with tone marks and corresponding no-tone vowels\n",
        "vowel_table = [['a', 'à', 'á', 'ả', 'ã', 'ạ', 'a'],\n",
        "               ['ă', 'ằ', 'ắ', 'ẳ', 'ẵ', 'ặ', 'aw'],\n",
        "               ['â', 'ầ', 'ấ', 'ẩ', 'ẫ', 'ậ', 'aa'],\n",
        "               ['e', 'è', 'é', 'ẻ', 'ẽ', 'ẹ', 'e'],\n",
        "               ['ê', 'ề', 'ế', 'ể', 'ễ', 'ệ', 'ee'],\n",
        "               ['i', 'ì', 'í', 'ỉ', 'ĩ', 'ị', 'i'],\n",
        "               ['o', 'ò', 'ó', 'ỏ', 'õ', 'ọ', 'o'],\n",
        "               ['ô', 'ồ', 'ố', 'ổ', 'ỗ', 'ộ', 'oo'],\n",
        "               ['ơ', 'ờ', 'ớ', 'ở', 'ỡ', 'ợ', 'ow'],\n",
        "               ['u', 'ù', 'ú', 'ủ', 'ũ', 'ụ', 'u'],\n",
        "               ['ư', 'ừ', 'ứ', 'ử', 'ữ', 'ự', 'uw'],\n",
        "               ['y', 'ỳ', 'ý', 'ỷ', 'ỹ', 'ỵ', 'y']]\n",
        "\n",
        "# Tone mark table\n",
        "tone_mark_table = ['', 'f', 's', 'r', 'x', 'j']\n",
        "\n",
        "# Mapping from Vietnamese vowels to their indices in the vowel table\n",
        "vowel_to_ids = {}\n",
        "\n",
        "for i in range(len(vowel_table)):\n",
        "    for j in range(len(vowel_table[i]) - 1):\n",
        "        vowel_to_ids[vowel_table[i][j]] = (i, j)\n",
        "\n",
        "# Function to normalize the position of tone marks in a Vietnamese word\n",
        "def normalize_vietnamese_word(word):\n",
        "    if not is_valid_vietnamese_word(word):\n",
        "        return word\n",
        "    chars = list(word)\n",
        "    tone_mark = 0\n",
        "    vowel_indices = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = vowel_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check 'qu'\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check 'gi'\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            tone_mark = y\n",
        "            chars[index] = vowel_table[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            vowel_indices.append(index)\n",
        "\n",
        "    if len(vowel_indices) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = vowel_to_ids.get(chars[1])\n",
        "                chars[1] = vowel_table[x][tone_mark]\n",
        "            else:\n",
        "                x, y = vowel_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = vowel_table[x][tone_mark]\n",
        "                else:\n",
        "                    chars[1] = vowel_table[5][tone_mark] if chars[1] == 'i' else vowel_table[9][tone_mark]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in vowel_indices:\n",
        "        x, y = vowel_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = vowel_table[x][tone_mark]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(vowel_indices) == 2:\n",
        "        if vowel_indices[-1] == len(chars) - 1:\n",
        "            x, y = vowel_to_ids[chars[vowel_indices[0]]]\n",
        "            chars[vowel_indices[0]] = vowel_table[x][tone_mark]\n",
        "        else:\n",
        "            x, y = vowel_to_ids[chars[vowel_indices[1]]]\n",
        "            chars[vowel_indices[1]] = vowel_table[x][tone_mark]\n",
        "    else:\n",
        "        x, y = vowel_to_ids[chars[vowel_indices[1]]]\n",
        "        chars[vowel_indices[1]] = vowel_table[x][tone_mark]\n",
        "\n",
        "    return ''.join(chars)\n",
        "\n",
        "# Function to check if a word is a valid Vietnamese word\n",
        "def is_valid_vietnamese_word(word):\n",
        "    chars = list(word)\n",
        "    vowel_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = vowel_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if vowel_index == -1:\n",
        "                vowel_index = index\n",
        "            else:\n",
        "                if index - vowel_index != 1:\n",
        "                    return False\n",
        "                vowel_index = index\n",
        "    return True\n",
        "\n",
        "# Function to normalize tone marks in a Vietnamese sentence\n",
        "def normalize_vietnamese_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Normalize a Vietnamese sentence to standard tone mark positions.\n",
        "    :param sentence: Input sentence in Vietnamese\n",
        "    :return: Normalized sentence\n",
        "    \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        cleaned_word = re.sub(r'(^\\p{P}*)([p{L}.]*\\p{L}+)(\\p{P}*$)', r'\\1/\\2/\\3', word).split('/')\n",
        "        if len(cleaned_word) == 3:\n",
        "            cleaned_word[1] = normalize_vietnamese_word(cleaned_word[1])\n",
        "        words[index] = ''.join(cleaned_word)\n",
        "    return ' '.join(words)\n"
      ],
      "metadata": {
        "id": "Etvo3GOl0Epi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "vncorenlp_path = \"/content/VnCoreNLP/VnCoreNLP-1.2.jar\"\n",
        "\n",
        "rdrsegmenter = VnCoreNLP(vncorenlp_path, annotators=\"wseg\", max_heap_size='-Xmx2g')\n",
        "\n",
        "def process_sentence(sentence):\n",
        "    lw = sentence.split()\n",
        "    return [w.lower() for w in lw if len(w)>1 or w.isdigit()]\n",
        "\n",
        "def process_text_file(input_file, rdrsegmenter):\n",
        "    \"\"\"Xử lý văn bản từ file input, loại bỏ ký tự đặc biệt đầu dòng và dấu câu.\"\"\"\n",
        "    processed_lines = []\n",
        "\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    for line in lines:\n",
        "        line = normalize_vietnamese_sentence(line)\n",
        "        line = re.sub(r'^[^\\w\\d]+', '', line.strip())\n",
        "        sentences = re.split(r'[.!?]', line)\n",
        "        for sentence in sentences:\n",
        "            cleaned_sentence = re.sub(r'(?<!\\S)[^\\w\\s-\\/](?!\\S)', '', sentence).strip()\n",
        "            if cleaned_sentence and len(cleaned_sentence) > 2:\n",
        "                tokenized_sentences = rdrsegmenter.tokenize(cleaned_sentence)\n",
        "                for tokenized_sentence in tokenized_sentences:\n",
        "                    processed_lines.append(process_sentence(' '.join(tokenized_sentence)))\n",
        "    return processed_lines"
      ],
      "metadata": {
        "id": "yuORFI-R0EtY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_sentences = process_text_file(path_data_test, rdrsegmenter)"
      ],
      "metadata": {
        "id": "btkIjXK60ExQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(list_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5qtn4Of0FGS",
        "outputId": "b7a87ebf-12ba-4fca-d4ce-a0e829e2361e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "164"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7djZlLO4Cn6",
        "outputId": "6aefb44b-7060-4fef-9cc7-46a2b80fd1b3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['bộ',\n",
              "  'y_tế',\n",
              "  'đã',\n",
              "  'có',\n",
              "  'quyết_định',\n",
              "  '2609',\n",
              "  'qđ-byt',\n",
              "  'ban_hành',\n",
              "  'hướng_dẫn',\n",
              "  'phòng',\n",
              "  'và',\n",
              "  'kiểm_soát',\n",
              "  'lây_nhiễm',\n",
              "  'covid-19',\n",
              "  'trong',\n",
              "  'cơ_sở',\n",
              "  'khám',\n",
              "  'bệnh',\n",
              "  'chữa',\n",
              "  'bệnh'],\n",
              " ['hướng_dẫn',\n",
              "  'phòng',\n",
              "  'và',\n",
              "  'kiểm_soát',\n",
              "  'lây_nhiễm',\n",
              "  'covid-19',\n",
              "  'trong',\n",
              "  'cơ_sở',\n",
              "  'khám',\n",
              "  'bệnh',\n",
              "  'chữa',\n",
              "  'bệnh',\n",
              "  'nêu',\n",
              "  'rõ',\n",
              "  'các',\n",
              "  'biện_pháp',\n",
              "  'kiểm_soát',\n",
              "  'lây_nhiễm',\n",
              "  'sars-cov',\n",
              "  '-2',\n",
              "  'trong',\n",
              "  'cơ_sở',\n",
              "  'khám',\n",
              "  'chữa',\n",
              "  'bệnh',\n",
              "  'như',\n",
              "  'sau'],\n",
              " ['tổ_chức',\n",
              "  'phát_hiện',\n",
              "  'sớm',\n",
              "  'và',\n",
              "  'cách_ly',\n",
              "  'kịp_thời',\n",
              "  'người',\n",
              "  'mắc',\n",
              "  'hoặc',\n",
              "  'nghi',\n",
              "  'mắc',\n",
              "  'covid-19'],\n",
              " ['người_bệnh',\n",
              "  'có',\n",
              "  'dấu_hiệu',\n",
              "  'chỉ_điểm',\n",
              "  'mắc',\n",
              "  'covid-19',\n",
              "  'ho',\n",
              "  'sốt',\n",
              "  'khi',\n",
              "  'đến',\n",
              "  'cơ_sở',\n",
              "  'khám',\n",
              "  'bệnh',\n",
              "  'chữa',\n",
              "  'bệnh',\n",
              "  'được',\n",
              "  'chỉ_dẫn',\n",
              "  'khám',\n",
              "  'tại',\n",
              "  'phòng_khám',\n",
              "  'truyền_nhiễm',\n",
              "  'người_bệnh',\n",
              "  'cấp_cứu',\n",
              "  'được',\n",
              "  'chỉ_dẫn',\n",
              "  'khám',\n",
              "  'tại',\n",
              "  'khu_vực',\n",
              "  'tiếp_đón',\n",
              "  'của',\n",
              "  'khoa',\n",
              "  'cấp_cứu'],\n",
              " ['thực_hiện', 'xét_nghiệm', 'chẩn_đoán', 'nhanh', 'covid-19']]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentences(sentences, dict_viSL):\n",
        "    encoded_sentences = []\n",
        "    total_words = 0  # Tổng số từ\n",
        "    total_zero_encoded = 0  # Tổng số từ được mã hóa thành 0\n",
        "\n",
        "    for sentence in sentences:\n",
        "        encoded_sentence = []\n",
        "        for word in sentence:\n",
        "            total_words += 1  # Đếm tổng số từ\n",
        "            if word.isdigit():  # Kiểm tra xem từ có phải là số không\n",
        "                encoded_sentence.append(\"Num\")\n",
        "            elif word in dict_viSL:  # Kiểm tra từ trong từ điển\n",
        "                encoded_sentence.append(dict_viSL[word])\n",
        "            else:\n",
        "                encoded_sentence.append(0)\n",
        "                total_zero_encoded += 1  # Đếm số từ được mã hóa thành 0\n",
        "        encoded_sentences.append(encoded_sentence)\n",
        "\n",
        "    return encoded_sentences, total_words, total_zero_encoded"
      ],
      "metadata": {
        "id": "5lawbpcY3BHM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_result, total_words, total_zero_encoded = encode_sentences(list_sentences, dict_viSL)\n",
        "\n",
        "print(f\"Encryption result: {encoded_result}\")\n",
        "print(f\"Total words: {total_words}\")\n",
        "print(f\"Total words encoded as 0: {total_zero_encoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMgRK6ci0FKH",
        "outputId": "43b39ea0-99e1-4398-cfa8-7e4b3e749a68"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encryption result: [[0, 0, 0, 783, 498, 'Num', 0, 0, 2446, 2635, 1300, 0, 1638, 0, 3944, 0, 2499, 2682, 0, 2682], [2446, 2635, 1300, 0, 1638, 0, 3944, 0, 2499, 2682, 0, 2682, 0, 1791, 0, 470, 0, 1638, 0, 0, 3944, 0, 2499, 0, 2682, 0, 0], [2264, 0, 3534, 1300, 2745, 0, 942, 2267, 0, 0, 2267, 0], [0, 783, 2882, 0, 2267, 0, 2561, 854, 2040, 166, 0, 2499, 2682, 0, 2682, 3334, 0, 2499, 0, 0, 0, 0, 3949, 3334, 0, 2499, 0, 0, 0, 0, 0, 3949], [0, 2505, 120, 4018, 0], [0, 0, 0, 0, 437, 1815, 3334, 2745, 1348, 0, 0, 3949, 3485, 0, 3944, 0, 3949, 0, 3334, 2745, 1348, 0, 0], [0, 0, 0, 0, 3485, 0, 0, 2267, 0, 0, 0, 0, 0, 0, 2745, 0, 0, 0, 2745, 1348], [0, 2499, 2682, 0, 1679, 0, 0, 0, 3949, 1300, 0, 0, 2745, 0, 0, 0, 0, 437, 0, 1242, 0, 0, 0, 1300, 3532, 783, 0, 0, 3597, 1969, 2745, 0, 0], [4180, 0, 1496, 0, 942, 2267, 0, 0, 2267, 0, 474, 0, 1067, 0, 1300, 524, 0, 0, 0, 3041, 2316], [0, 0, 470, 0, 0, 0, 0, 0, 0, 3792, 0], [0, 0, 0, 0, 0, 0, 0, 3792, 3332, 3792, 0, 1300, 3792, 0, 2547, 0, 0, 0, 0, 0, 2040, 0, 1348, 309, 942, 2267, 0, 0, 2267, 0], [0, 0, 3567, 0, 942, 2682], [0, 2446, 0, 0, 0, 0, 3232, 0, 1537, 2166, 2040, 2561, 622, 3402, 2316, 0, 0, 2040, 0, 0, 1679, 1300, 0, 1067, 0, 0, 3377, 0, 4195, 0, 0], [0, 0, 0, 3792, 0], [0, 0, 0, 3792, 3332, 0, 0, 3567, 2040, 0, 0, 0, 783, 0, 0, 0, 0, 2040, 0, 942, 2267, 0, 0, 2267, 0], [4180, 0, 474, 0, 1067, 0], [0, 0, 0, 3792, 0, 2547, 0, 0, 1300, 0, 0, 0, 3567, 0, 0, 0, 0], [0, 3334, 0, 0, 0, 2745, 0, 477, 0, 0, 0, 0, 3024], [4180, 0, 0, 1067, 0, 1300, 524, 2040, 0, 309, 0, 0], [0, 176, 211, 3377, 0, 1836, 2040, 0, 0, 0, 783, 0, 0, 2547, 0, 690], [0, 0, 0, 0, 0, 3041, 2316], [0, 0, 0, 3532, 0, 1067, 0, 361, 2531, 2040, 1008, 0, 4322, 0, 2745], [0, 0, 0, 1396, 0, 309, 942, 2682], [0, 0, 2745, 474, 0, 0, 0, 0], [3485, 4315, 783, 0, 0, 0, 0, 3567, 0, 0, 2902, 0, 0, 0, 0, 0], [0, 0, 4065, 3332, 'Num', 0], [0, 0, 0, 0, 4322, 0, 2745, 474, 0, 0, 0, 2595, 942, 0], [0, 0, 0, 2902, 0, 0, 309, 942, 2682], [3377, 1969, 0, 3438], [1760, 4180, 0, 2040, 0, 0, 1414, 0, 309, 0, 0, 0, 4195, 0, 919, 0, 3377, 0, 1316, 1969, 0, 3438, 0, 0, 3765, 0, 0, 1638], [0, 0, 0, 0, 0, 1067, 2040, 0, 0, 2499, 2682, 0, 2682], [3942, 3377, 0, 1969, 0, 3438, 3942, 0, 0, 0, 0], [3942, 3377, 0, 1969, 0, 3438, 2694, 0, 0], [0, 1969, 0, 3438, 4195, 0, 0], [0, 180, 3041, 2316], [0, 0, 2499, 2682, 0, 2682, 474, 0, 0, 3041, 2316, 0, 2446, 3041, 0, 0, 0, 2499, 2682, 0, 2682, 0, 0, 498, 2455, 'Num', 0, 2572, 'Num', 0, 0, 0], [474, 0, 0, 3481, 0], [0, 0, 1969, 3041, 2316, 361, 2531, 0, 0, 783, 0, 0, 0, 0, 2499, 2682, 0, 2682, 0, 0, 0, 0], [0, 0, 3041, 2316, 783, 0, 0, 0, 0, 0, 0, 0, 0, 1300, 0, 3232, 3041, 2316, 2040, 474], [0, 0, 0, 180, 3041, 2316, 4180, 0, 0, 1300, 0, 3232, 361, 0, 783, 0, 1638, 2065, 0, 2745], [3041, 0, 0, 0, 1396, 0, 1300, 1969, 840, 309, 942, 2682], [474, 0, 3041, 0, 0, 0, 1396, 0, 1300, 1969, 840, 309, 0, 0, 2446, 3041, 1396, 0, 3944, 0, 0, 2499, 2682, 0, 2682, 0, 0, 498, 2455, 'Num', 0, 2572, 'Num', 0, 0, 0], [474, 0, 0, 789, 0], [4180, 0, 0, 3041, 0, 0, 1396, 0, 4180, 0, 1300, 4180, 3041, 0, 474, 3334, 0, 0, 0, 0, 3041, 0, 0, 1396, 2040, 1464, 0, 0], [0, 2499, 2682, 0, 2682, 474, 0, 0, 1969, 0, 0, 0, 1969, 0, 3438, 4054, 0, 3041, 0, 0, 0, 1396, 1969, 840, 1300, 309, 942, 2682], [0, 0, 3413, 3041, 0, 0, 0, 0, 2338, 783, 0, 2619, 0, 0, 0, 2745, 0, 0], [0, 1638, 0, 0, 0, 0, 361], [0, 0, 0], [0, 1242, 0, 0, 0, 0, 0], [0, 4180, 0, 0, 0, 0], [0, 0, 0, 0, 1067, 0, 1300, 176, 0, 0, 2971, 0, 2218, 0], [134, 0, 783, 0, 1638, 2065, 0, 0, 508, 0, 2218, 0, 1156], [0, 0, 0, 0, 903, 0, 3597, 0], [0, 0, 0, 0, 0, 3944, 3578, 1348], [2446, 0, 0, 942, 309, 0, 1300, 0, 3232, 0, 3041, 2316, 3041, 1679, 2040, 2561, 3041, 1969, 3438, 1300, 0, 1067, 0], [1737, 0, 0, 0, 2635, 0, 0, 616, 0, 1300, 0, 2446, 0, 0, 0, 0, 2635, 0, 0, 0, 0], [0, 0, 3534, 0, 2882, 0, 2267, 0, 0, 120, 1300, 1348, 0, 0], [4180, 0, 0, 0, 180, 0, 0, 1300, 0, 0, 3792, 0, 2040, 309, 0, 0], [0, 0, 2635, 0], [4180, 0, 474, 3334, 0, 0, 2635, 0, 0, 2446, 0, 0, 0, 0, 2635, 0, 0, 0, 0], [0, 0, 1300, 0, 0, 0, 180, 0, 4180, 0], [0, 818, 0, 2499, 2682, 0, 2682, 474, 2264, 0, 0, 4054, 4180, 0, 2894, 0, 4180, 3476, 1300, 1799, 0, 0, 0, 180, 0, 0, 0, 0, 180, 0, 0, 2682, 0, 0, 0], [0, 922, 0, 4157, 0, 0, 0, 180, 0, 1638, 4180, 0, 1300, 0, 0, 0, 789, 0, 0, 0], [3731, 0, 919, 2403, 4281], [0, 0], [0, 1221, 1300, 0, 0, 'Num', 0, 0, 0, 0, 3784, 3397, 0, 0, 2702, 4025, 0, 0, 3082, 1300, 783, 2346, 2345], [0, 2040, 0, 0, 0, 0, 0, 0, 1300, 0, 0, 1300, 0, 0, 'Num', 0, 2438], [3402, 1819, 381, 2412, 1802, 1300, 0, 546, 2895, 0, 3275, 4317, 3189, 'Num', 0, 0, 0], [3485, 4053, 0, 381, 0, 382, 3397, 3402, 381, 0, 0, 1300, 2438, 0, 2358, 0, 2857, 0, 0, 0, 4054, 381, 0, 0, 0, 0, 460, 0, 0, 469, 1300, 2727, 0, 1469, 0, 381, 1745, 0, 1300, 783, 4302, 2658, 3572], [0, 0, 3189, 0, 'Num', 0, 0, 2358, 3402, 1819, 1300, 2184, 0, 2708, 3780, 1300, 2129, 0, 0, 0, 0, 2345, 0, 2938, 0, 2438], [3402, 0, 1819, 'Num', 4226, 2702, 0, 3447, 0, 4222, 2845, 1483, 0, 364, 4054, 0, 2857], [0, 2358, 0, 938, 0, 0, 0, 0, 2159, 2702, 903, 0], [0, 0, 2798, 1469, 382, 2382, 0, 0, 2702, 2300, 3942, 2403, 1633, 2702, 2531, 0, 2382, 514, 4311, 2702, 0, 0, 0, 0], [0, 0, 4281, 337, 2316], [919, 'Num', 1486, 'Num', 2702, 0, 0, 0, 0, 437, 1133, 0, 2694, 547, 0, 'Num', 2702, 3024, 4172, 0, 0, 0, 3380, 2300, 4226, 437, 0, 0, 0], [919, 'Num', 4054, 0, 0, 4311, 0, 2702, 546, 1486, 0, 3189, 0, 0, 4076], [919, 'Num', 0, 2358, 4054, 0, 0, 0, 546, 2895, 0, 'Num', 166, 'Num', 0, 381, 2412, 1802, 546, 0, 0, 0, 3583, 0, 1682, 0, 1300, 2092, 2531, 0, 0, 0, 0, 0, 0, 0, 2683, 1300, 381], [919, 'Num', 2013, 0, 0, 2702, 0, 0, 437, 1300, 0, 0], [0, 0, 437, 0, 0, 4281, 3476, 2936, 3334], [1474, 0, 0, 3924, 0, 2702, 0, 0, 3944, 3485, 3924, 720, 3397, 0, 0, 2382, 2587], [919, 'Num', 0, 'Num', 0, 0, 0, 2316, 0, 2702, 0, 0, 0, 3380, 0, 3944, 3447, 0, 0, 3944, 2040, 0, 0, 0, 0, 0, 0, 2702, 0, 0, 0, 0], [2403, 0, 0, 4217, 2719, 0], [919, 'Num', 0, 2040, 3275, 3334, 0, 0, 2477, 0, 0, 3275, 0, 3222, 'Num', 2477, 0, 0, 1336, 0, 0, 0, 0, 2232, 3275, 1300, 3942, 1443, 0, 2232], [0, 166, 382, 0, 0, 'Num', 0, 0, 0, 0, 'Num', 0, 3380], [382, 4172, 3397, 0, 2477, 2694, 2702, 0, 0, 514, 919, 353, 0, 3215, 4054, 3275, 1443, 0, 0, 2040, 0], [0, 3275, 337, 0], [919, 'Num', 1486, 'Num', 2702, 0, 2151, 0, 1486, 0, 0, 337, 2316, 'Num', 2702, 2694, 1133, 4226, 437, 0, 'Num', 2702, 3380, 0, 4226, 437, 0], [1133, 0, 0, 3944, 0, 0, 4311, 2702], [919, 'Num', 0, 0, 0, 437, 0, 0, 'Num', 0, 0, 3215, 0, 3275, 1443, 0, 4054, 0, 0, 3447, 3583, 381, 0, 2358, 0, 3583, 0, 1682, 0, 2092, 2531, 0, 0, 0, 0], [919, 'Num', 4237, 0, 2702, 0, 0, 'Num', 2316, 0, 2166, 0, 3447, 0, 0, 0, 3222, 2477, 0, 'Num', 0, 0, 0, 0], [3942, 0, 2477, 0, 2232, 0, 3215, 3275, 2009, 3942, 2936, 1745], [0, 2647, 4281, 0, 0, 3082, 1745], [0, 3397, 3334, 0, 0, 3275, 1745, 1300, 2936, 0, 0, 0, 0, 0, 0, 3275, 0, 0, 437, 4314, 0, 0, 3275], [0, 2531, 0, 4021, 1300, 4161, 658], [2647, 3275, 337, 2025, 0], [919, 'Num', 353, 1486, 4281, 0, 0, 0, 3303, 0, 2857, 0, 4226, 3275, 0, 2040, 1486, 4281, 0, 3303, 382, 474, 1486, 0, 1682, 0, 2702, 0, 0, 2694, 0, 3275, 3942, 1443, 1576, 1300, 0, 4267, 3303], [799, 3275, 0, 0, 0, 0, 0, 0, 0, 3275, 382, 2403, 0, 0, 3275, 2798, 0, 3189, 'Num', 0], [919, 'Num', 2040, 0, 3275, 3485, 1896, 2857, 0, 0, 437, 0, 3583, 2857, 0, 0, 4054, 0, 4226, 3275, 0, 3275, 0, 0], [0, 0, 382, 437, 1992, 2857, 0, 0, 0, 0, 3942, 858, 0, 2857, 3848], [2040, 0, 2647, 3334, 2938, 799, 437, 2794, 0, 0, 3275, 0, 0, 3275, 3334, 0, 0], [919, 'Num', 0, 2040, 3275, 0, 382, 3222, 0, 0, 3303, 4054, 3275, 0, 2857, 3848, 0, 0, 'Num', 4078, 3447, 1486, 3275, 0, 2211, 1486, 3275, 0, 720, 1682, 0, 0, 1309, 3827, 0, 0, 2078, 3275, 4054, 0, 2857, 3275, 0, 0, 0, 1300, 0, 0, 3944, 0, 2707, 799, 2078, 1037, 2857, 514, 3189, 'Num', 166, 'Num', 0], [2647, 4281, 337, 3303, 3914], [0, 0, 0, 0, 0, 1760, 0, 0, 2025, 0, 2995, 199, 942, 0, 3942, 783, 0, 2025, 3397, 3765, 3377, 0, 720, 2707], [361, 3427, 0, 0, 0, 1088, 4281, 0, 3736], [919, 'Num', 3303, 3914, 0, 4281, 0, 0, 3526, 858, 4054, 799, 3275, 0, 4018, 0], [382, 0, 2702, 4054, 3275, 0, 3303, 0, 2857, 0, 166, 0, 0, 0, 3303], [919, 'Num', 382, 0, 0, 0, 0, 3914, 1300, 0, 1037, 0, 0, 3303], [2040, 2857, 0, 0, 0, 0, 546, 1300, 1518, 0, 0, 2857, 4054, 0, 2040, 3275, 0, 'Num', 'Num', 0, 0, 0, 1300, 0, 3275], [919, 'Num', 0, 2040, 3275, 0, 0, 0, 0, 2403, 2151, 0, 2647, 3275, 337, 0], [0, 2531, 382, 0, 0, 0, 2647, 4281, 0, 1222, 0, 1467], [4281, 1745, 0, 2936, 382, 3397, 0, 0, 3380, 0, 0, 121, 0, 0, 0, 2755, 4054, 0, 1590], [2040, 0, 3397, 0, 3583, 0, 1682, 2702, 4322, 3275, 4054, 2936], [4281, 2914, 0, 0, 0, 1370, 1606, 0, 0, 0, 0, 0, 2914, 0, 2029, 0, 1745], [0, 0, 0, 0, 1300, 0, 799, 2300, 0, 3041, 3765, 0, 0, 1469, 0, 882, 3583, 0, 0, 1239, 2104, 3899, 0, 210, 0, 3332, 1656, 166, 1683, 0, 0, 0, 942], [0, 0, 4281, 3334, 4161], [3485, 1642, 3848, 986, 4281, 2403, 0, 1158, 3942, 1370, 3397, 0, 3708, 922, 877, 2635], [1874, 0, 3275, 0, 474, 0, 337, 0, 0, 0, 0, 3334, 3189, 'Num', 2572, 877, 2635], [3485, 3618, 0, 4161, 0, 2040, 2078, 382, 0, 4054, 2129, 3275, 0, 3447, 0, 0, 0, 3944, 0, 0, 0, 0, 0, 0, 0, 3421, 3334, 'Num', 922], [3618, 0, 4161, 3467, 0, 382, 0, 2766], [3944, 0, 799, 0, 3275, 382, 474, 1486, 'Num', 1682, 2702, 0, 0, 2694, 0, 3275, 3942, 1443, 1576, 1300, 0, 4267, 3303], [1486, 3275, 0, 0, 0, 1321, 0, 0, 1300, 0, 0, 0, 3275, 3334, 0, 2635, 2040, 2857, 0, 0, 1211, 2403, 3275, 1443, 0, 0, 2299, 2724], [0, 2040, 2857, 0, 0, 382, 437, 0, 4145, 0, 2025, 0, 0, 1404, 0, 0, 0, 0, 2025, 3914], [1802, 0, 4145, 0, 3944, 0, 3578, 0, 3275], [2177, 3054, 2531, 0, 0, 0, 0, 942, 2040, 190, 3054, 3380, 0, 0, 2857], [0, 2572, 3380, 0, 2857, 0, 0, 0, 0, 0, 2177, 3054, 2403, 3573, 1300, 0, 0, 720, 942], [66, 0, 0, 2531, 0, 1403, 4054, 0, 1300, 3583, 0, 1993, 1309, 4054, 2858, 1300, 0, 0, 0, 0, 2995, 1300, 2858], [0, 0, 2358, 0, 0, 2065, 0, 4149, 0, 3765, 3054, 0, 2403, 3785, 316, 0, 0, 4149, 0, 942, 190, 3054, 3532, 3334, 3012, 2531, 3142, 4021, 3944, 0, 0, 0, 3765, 3054], [0, 942, 2587, 0, 2587, 4195, 0, 3792, 0, 0, 4054, 942, 2587, 0], [437, 2587, 3380, 3519, 0, 1128, 0, 3792, 3942, 783, 3519], [2040, 0, 3792, 437, 3345, 0, 0, 630, 0, 2587, 0, 1300, 1802, 0, 3792, 2040, 0, 3765], [0, 0, 1379, 166, 0, 337, 171, 437, 0, 4195, 2217, 3054, 0, 4054, 942, 2587, 171], [942, 1951, 630, 1300, 942, 4329, 0, 630, 437, 0, 0, 1300, 783, 0, 0, 4195, 0], [0, 858, 0, 3944, 912, 1300, 4054, 1379, 0, 0, 0, 0, 0, 0, 3765, 0, 4054, 912, 1300, 0], [1951, 2056, 437, 0, 0, 0, 3054, 0, 942, 2587, 2056, 1300, 1951, 630, 3380, 0, 0, 437, 3944, 3867], [0, 0, 0, 3700, 0, 0, 942, 1951, 3054, 3345, 0, 2040, 1008, 0], [2040, 2587, 2056, 3914, 2301, 474, 437, 0, 0, 2040, 190, 3054, 0, 0, 4195, 0], [0, 4155, 3765, 0, 630, 0, 0, 0, 3765, 0, 1462], [0, 630, 3765, 2040, 190, 3054, 2587, 4195, 0, 0, 0, 4054, 2056, 0, 2056, 3914], [3942, 3334, 0, 0, 3792, 0, 4054, 347, 3942, 2587, 0, 0, 3792, 0, 4054, 942, 2587, 0, 3942, 1527, 4018, 0, 468, 0, 0, 3942, 2587, 630, 2188, 0, 2455, 942, 0], [474, 437, 0, 4195, 2217, 3942, 3334, 2587, 3792, 0, 965, 0, 0, 2587, 4195, 0, 1300, 0], [0, 0, 2358, 474, 0, 3526, 0, 0, 1848, 3765, 3054, 0, 0, 0, 0, 3792, 0, 0, 2766, 0, 0, 1300, 0, 0, 3942, 0, 0, 0, 1300, 0, 1848, 3765, 3054, 0, 0, 0, 0, 0, 0, 0, 3792, 0], [3942, 0, 529, 3519, 0, 2617, 2212, 0, 2403, 1779, 0], [0, 0, 2358, 0, 0, 0, 0, 3142, 818, 3765, 3054, 'Num', 0, 0, 0, 0, 0, 3592, 1300, 0, 0, 474, 0, 0, 0, 0, 3792, 3054, 0, 0, 789, 3750, 0, 0, 0, 4315, 0, 0, 0, 0, 0, 0, 783, 470, 0], [3012, 0, 0, 0, 3765, 3054, 2531, 1204, 0, 0, 942, 0], [0, 0, 0, 0, 0, 2446, 0, 2217, 3054, 474, 361, 586, 0, 2065, 0, 4054, 942, 883, 0, 171, 630, 783, 0, 0], [0, 0, 1443, 593, 2040, 190, 3054, 0, 2217, 3054, 0, 3397, 3269, 0, 0, 0, 0, 0, 3792, 1077, 1256, 0, 1075, 0, 0, 3792, 0, 1443, 3444], [0, 630, 0, 0, 630, 2264, 0, 630, 0, 0, 0], [1951, 1969, 3054, 0, 1300, 3944, 690, 783, 3070, 0], [1951, 630, 347, 1807, 171, 0, 3380, 3792, 0, 3944, 2218, 0, 0, 0, 783, 0, 0], [1951, 0, 3942, 783, 0, 883, 0, 0], [0, 45, 0, 3944, 0, 190, 3054, 0, 45, 0, 0, 0], [0, 0, 'Num', 0, 'Num', 2217, 3054, 0, 593, 0, 1951, 0, 1021, 0, 0, 0, 0, 3792, 0, 468], [3485, 942, 190, 3054, 0, 0, 3380, 0, 1443, 3252, 1146, 514, 'Num'], ['Num', 166, 'Num', 2763, 0, 0, 942, 1951, 630, 0], [0, 1951, 630, 1021, 0, 0], [45, 3377, 629, 2395, 2040, 190, 3054, 0, 1443, 3252, 1146, 514, 'Num', 2763, 0, 166, 'Num', 2763, 0, 0, 0, 0, 0, 3944, 2218], [0, 0, 0, 2177, 3054, 134, 0, 0, 0, 912, 1300, 0, 2858], [0, 456, 1037, 0, 0, 2040, 190, 3054, 0, 0, 1300, 659, 4149, 2217, 3054], [0, 474, 783, 0, 0, 720, 4226, 1300, 0, 0, 0, 1027, 4157, 1300, 0, 0, 0, 0, 0, 0], [3142, 1328, 2177, 3054, 3944, 799, 0, 3476, 1219, 0], [474, 0, 0, 2050, 0, 3942, 883], [0, 0, 2214, 2531, 0], [0, 3054, 0, 0, 3765, 0, 0, 1300, 1057], [3765, 3054, 1204, 0, 0, 942]]\n",
            "Total words: 2788\n",
            "Total words encoded as 0: 1336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HCYa51OhUEhi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "POS_TAG_ABBREVIATIONS = {\n",
        "    'adj': 'A',\n",
        "    'noun': 'N',\n",
        "    'verb': 'V',\n",
        "    'adv': 'ADV'\n",
        "}\n",
        "\n",
        "CATEGORY_ABBREVIATIONS = {\n",
        "    'color': 'C',\n",
        "    'size': 'S',\n",
        "    'motion': 'M',\n",
        "    'contact': 'CT',\n",
        "    'state': 'ST',\n",
        "    'artifact': 'ART',\n",
        "    'body': 'BD',\n",
        "    'group': 'GR',\n",
        "    'communication': 'COM',\n",
        "    'perception': 'PERC',\n",
        "    'possession': 'POS',\n",
        "    'social': 'SOC',\n",
        "    'stative': 'STV'\n",
        "}\n",
        "# Lớp Category để chứa danh sách từ đồng nghĩa\n",
        "class Category:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.synonyms = []\n",
        "\n",
        "    def add_synonyms(self, synonyms):\n",
        "        self.synonyms.append(synonyms)\n",
        "\n",
        "    def get_synonyms(self):\n",
        "        return self.synonyms\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Category(name={self.name}, synonyms={self.synonyms})\"\n",
        "\n",
        "# Lớp POSTag để chứa nhãn từ loại và danh mục\n",
        "class POSTag:\n",
        "    def __init__(self, pos_name):\n",
        "        self.pos_name = pos_name\n",
        "        self.categories = {}  # Từ điển {category_name: Category}\n",
        "        self.synonyms = []    # Lưu những từ đồng nghĩa nếu không có danh mục\n",
        "\n",
        "    def add_category(self, category_name):\n",
        "        if category_name not in self.categories:\n",
        "            self.categories[category_name] = Category(category_name)\n",
        "\n",
        "    def add_synonyms_to_category(self, category_name, synonyms):\n",
        "        if category_name in self.categories:\n",
        "            self.categories[category_name].add_synonyms(synonyms)\n",
        "\n",
        "    def add_synonyms(self, synonyms):\n",
        "        self.synonyms.append(synonyms)\n",
        "\n",
        "    def get_all_synonyms(self):\n",
        "        # Lấy tất cả từ đồng nghĩa từ cả danh mục và không danh mục\n",
        "        all_synonyms = self.synonyms[:]\n",
        "        for category in self.categories.values():\n",
        "            all_synonyms.extend(category.get_synonyms())\n",
        "        return all_synonyms\n",
        "\n",
        "    def get_synonyms_by_category(self, category_name):\n",
        "        if category_name in self.categories:\n",
        "            return self.categories[category_name].get_synonyms()\n",
        "        return []\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"POSTag(pos_name={self.pos_name}, categories={self.categories}, synonyms={self.synonyms})\"\n",
        "\n",
        "class Words:\n",
        "    def __init__(self):\n",
        "        self.pos_tags = {}  # Từ điển {pos_tag_name: POSTag}\n",
        "\n",
        "    def add_pos_tag(self, pos_tag_name):\n",
        "        if pos_tag_name not in self.pos_tags:\n",
        "            self.pos_tags[pos_tag_name] = POSTag(pos_tag_name)\n",
        "\n",
        "    def add_category_to_pos_tag(self, pos_tag_name, category_name):\n",
        "        if pos_tag_name in self.pos_tags:\n",
        "            self.pos_tags[pos_tag_name].add_category(category_name)\n",
        "\n",
        "    def add_synonyms_to_category(self, pos_tag_name, category_name, synonyms):\n",
        "        if pos_tag_name in self.pos_tags:\n",
        "            self.pos_tags[pos_tag_name].add_synonyms_to_category(category_name, synonyms)\n",
        "\n",
        "    def add_synonyms_to_pos_tag(self, pos_tag_name, synonyms):\n",
        "        if pos_tag_name in self.pos_tags:\n",
        "            self.pos_tags[pos_tag_name].add_synonyms(synonyms)\n",
        "\n",
        "    def get_synonyms_by_pos_tag(self, pos_tag_name):\n",
        "        if pos_tag_name in self.pos_tags:\n",
        "            return self.pos_tags[pos_tag_name].get_all_synonyms()\n",
        "        return []\n",
        "\n",
        "    def get_synonyms_by_category(self, pos_tag_name, category_name):\n",
        "        if pos_tag_name in self.pos_tags:\n",
        "            return self.pos_tags[pos_tag_name].get_synonyms_by_category(category_name)\n",
        "        return []\n",
        "\n",
        "    # Phương thức tìm kiếm thông tin của một từ\n",
        "    def find_word(self, word):\n",
        "        results = []\n",
        "        word = word.replace(' ', '_')  # Đảm bảo từ cần tìm có dạng đúng (_ thay vì khoảng trắng)\n",
        "        for pos_tag_name, pos_tag in self.pos_tags.items():\n",
        "            # Kiểm tra từ trong danh sách từ đồng nghĩa không có danh mục\n",
        "            for synonym_group in pos_tag.synonyms:\n",
        "                if word in synonym_group:\n",
        "                    results.append(f\"Từ '{word}' thuộc POS-TAG '{pos_tag_name}' và không có danh mục.\")\n",
        "\n",
        "            # Kiểm tra từ trong các danh mục\n",
        "            for category_name, category in pos_tag.categories.items():\n",
        "                for synonym_group in category.get_synonyms():\n",
        "                    if word in synonym_group:\n",
        "                        results.append(f\"Từ '{word}' thuộc POS-TAG '{pos_tag_name}' trong danh mục '{category_name}'.\")\n",
        "\n",
        "        if not results:\n",
        "            return f\"Từ '{word}' không tìm thấy.\"\n",
        "        return \"\\n\".join(results)\n",
        "\n",
        "    # Phương thức tạo từ điển từ - {pos-tag, category} với ký hiệu ngắn gọn\n",
        "    def generate_word_dict(self):\n",
        "        word_dict = {}\n",
        "\n",
        "        # Duyệt qua tất cả các POS-TAG\n",
        "        for pos_tag_name, pos_tag in self.pos_tags.items():\n",
        "            # Duyệt qua từ đồng nghĩa không có danh mục\n",
        "            for synonym_group in pos_tag.synonyms:\n",
        "                for word in synonym_group:\n",
        "                    if word not in word_dict:\n",
        "                        word_dict[word] = []\n",
        "                    abbrev_pos_tag = POS_TAG_ABBREVIATIONS.get(pos_tag_name, pos_tag_name)\n",
        "                    word_dict[word].append({'pos_tag': abbrev_pos_tag, 'category': None})\n",
        "\n",
        "            # Duyệt qua từ đồng nghĩa có danh mục\n",
        "            for category_name, category in pos_tag.categories.items():\n",
        "                for synonym_group in category.get_synonyms():\n",
        "                    for word in synonym_group:\n",
        "                        if word not in word_dict:\n",
        "                            word_dict[word] = []\n",
        "                        abbrev_pos_tag = POS_TAG_ABBREVIATIONS.get(pos_tag_name, pos_tag_name)\n",
        "                        abbrev_category = CATEGORY_ABBREVIATIONS.get(category_name, category_name)\n",
        "                        word_dict[word].append({'pos_tag': abbrev_pos_tag, 'category': abbrev_category})\n",
        "\n",
        "        return word_dict\n",
        "    def generate_numbered_synonyms_dict(self):\n",
        "        numbered_synonyms_dict = {}\n",
        "        current_key = 1\n",
        "\n",
        "        # Duyệt qua tất cả các POS-TAG\n",
        "        for pos_tag_name, pos_tag in self.pos_tags.items():\n",
        "            # Xử lý các từ đồng nghĩa không có danh mục\n",
        "            for synonym_group in pos_tag.synonyms:\n",
        "                numbered_synonyms_dict[current_key] = {\n",
        "                    'synonyms': synonym_group,\n",
        "                    'info': [pos_tag_name, None]  # Không có danh mục\n",
        "                }\n",
        "                current_key += 1\n",
        "\n",
        "            # Xử lý các từ đồng nghĩa có danh mục\n",
        "            for category_name, category in pos_tag.categories.items():\n",
        "                for synonym_group in category.get_synonyms():\n",
        "                    numbered_synonyms_dict[current_key] = {\n",
        "                        'synonyms': synonym_group,\n",
        "                        'info': [pos_tag_name, category_name]\n",
        "                    }\n",
        "                    current_key += 1\n",
        "\n",
        "        return numbered_synonyms_dict\n",
        "    def find_keys_by_word(self, word, numbered_synonyms_dict):\n",
        "        matching_keys = []\n",
        "        word = word.replace(' ', '_')  # Đảm bảo từ có định dạng dùng dấu gạch dưới\n",
        "\n",
        "        # Duyệt qua từ điển và tìm các key mà danh sách từ đồng nghĩa chứa từ cần tìm\n",
        "        for key, value in numbered_synonyms_dict.items():\n",
        "            if word in value['synonyms']:\n",
        "                matching_keys.append(key)\n",
        "\n",
        "        if not matching_keys:\n",
        "            return f\"Từ '{word}' không xuất hiện trong danh sách.\"\n",
        "        return matching_keys\n",
        "    def generate_synonyms_dict(self):\n",
        "        synonyms_dict = {}\n",
        "\n",
        "        # Duyệt qua tất cả các POS-TAG\n",
        "        for pos_tag_name, pos_tag in self.pos_tags.items():\n",
        "            # Xử lý các từ đồng nghĩa không có danh mục\n",
        "            for synonym_group in pos_tag.synonyms:\n",
        "                if tuple(synonym_group) not in synonyms_dict:\n",
        "                    synonyms_dict[tuple(synonym_group)] = []\n",
        "                synonyms_dict[tuple(synonym_group)].append({\n",
        "                    'pos_tag': pos_tag_name,\n",
        "                    'category': None\n",
        "                })\n",
        "\n",
        "            # Xử lý các từ đồng nghĩa có danh mục\n",
        "            for category_name, category in pos_tag.categories.items():\n",
        "                for synonym_group in category.get_synonyms():\n",
        "                    if tuple(synonym_group) not in synonyms_dict:\n",
        "                        synonyms_dict[tuple(synonym_group)] = []\n",
        "                    synonyms_dict[tuple(synonym_group)].append({\n",
        "                        'pos_tag': pos_tag_name,\n",
        "                        'category': category_name\n",
        "                    })\n",
        "\n",
        "        return synonyms_dict\n",
        "    # Phương thức liệt kê tất cả các từ trong class Words\n",
        "    def list_all_words(self):\n",
        "        all_words = set()\n",
        "\n",
        "        # Duyệt qua tất cả các POS-TAG\n",
        "        for pos_tag_name, pos_tag in self.pos_tags.items():\n",
        "            for synonym_group in pos_tag.synonyms:\n",
        "                all_words.update(synonym_group)\n",
        "\n",
        "            for category_name, category in pos_tag.categories.items():\n",
        "                for synonym_group in category.get_synonyms():\n",
        "                    all_words.update(synonym_group)\n",
        "\n",
        "        return all_words\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Words(pos_tags={self.pos_tags})\"\n",
        "\n",
        "def process_file(file_path, words_data, setWords):\n",
        "    file_name = os.path.basename(file_path)\n",
        "    name_parts = file_name.split('.')\n",
        "\n",
        "    if len(name_parts) == 3:\n",
        "        pos_tag = name_parts[0]\n",
        "        category = name_parts[1]\n",
        "\n",
        "        words_data.add_pos_tag(pos_tag)\n",
        "        words_data.add_category_to_pos_tag(pos_tag, category)\n",
        "\n",
        "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            for row in reader:\n",
        "                synonyms = [word.strip().replace(' ', '_').lower() for word in row if word.strip()]\n",
        "                if any(word in setWords for word in synonyms):\n",
        "                    words_data.add_synonyms_to_category(pos_tag, category, synonyms)\n",
        "\n",
        "    elif len(name_parts) == 2:\n",
        "        pos_tag = name_parts[0]\n",
        "\n",
        "        words_data.add_pos_tag(pos_tag)\n",
        "\n",
        "        with open(file_path, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.reader(csvfile)\n",
        "            for row in reader:\n",
        "                synonyms = [word.strip().replace(' ', '_').lower() for word in row if word.strip()]\n",
        "                if any(word in setWords for word in synonyms):\n",
        "                    words_data.add_synonyms_to_pos_tag(pos_tag, synonyms)\n",
        "\n",
        "def process_directory(directory_path, setWords):\n",
        "    words_data = Words()\n",
        "\n",
        "    for file_name in os.listdir(directory_path):\n",
        "        if file_name.endswith('.csv'):\n",
        "            file_path = os.path.join(directory_path, file_name)\n",
        "            process_file(file_path, words_data, setWords)\n",
        "\n",
        "    return words_data\n",
        "directory_path = '/content/drive/MyDrive/Data_paper_4/Dict'\n",
        "words_data = process_directory(directory_path, set_words)\n",
        "#print(words_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYBnAiDXUEhm",
        "outputId": "06dc0c14-513a-4cb5-e29e-6251f21c6ef0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tất cả các từ trong Words:\n",
            "cú_đấm\n",
            "cảnh_nghèo\n",
            "gian_khổ\n",
            "cản_phá\n",
            "suy_ngẫm\n",
            "gảy_đàn\n",
            "ngất_xỉu\n",
            "đáng_tin\n",
            "khí_đốt\n",
            "yêu_quý\n"
          ]
        }
      ],
      "source": [
        "# Giả sử bạn đã thêm dữ liệu vào words_data\n",
        "all_words = words_data.list_all_words()\n",
        "\n",
        "# In ra tất cả các từ\n",
        "print(\"Tất cả các từ trong Words:\")\n",
        "c = 0\n",
        "for word in all_words:\n",
        "    if c< 10:\n",
        "\n",
        "        print(word)\n",
        "        c+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-txTqMKgUEho",
        "outputId": "f7ac5783-0cb0-4cb8-fbcc-56e5545055a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16627"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "len(all_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3psXBuWaUEhp",
        "outputId": "989b65f2-e5ed-4b7a-e64e-89267d8310cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for word, info in word_dict.items():\\n    print(f\"Từ: {word}\")\\n    for entry in info:\\n        pos_tag = entry[\\'pos_tag\\']\\n        category = entry[\\'category\\'] if entry[\\'category\\'] else \\'None\\'\\n        print(f\"  - POS-TAG: {pos_tag}, Category: {category}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "word_dict = words_data.generate_word_dict()\n",
        "'''for word, info in word_dict.items():\n",
        "    print(f\"Từ: {word}\")\n",
        "    for entry in info:\n",
        "        pos_tag = entry['pos_tag']\n",
        "        category = entry['category'] if entry['category'] else 'None'\n",
        "        print(f\"  - POS-TAG: {pos_tag}, Category: {category}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "3puCq268UEht",
        "outputId": "b0b4779c-0fe0-47b3-9058-a12935cbfbe2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for synonyms, info in synonyms_dict.items():\\n    print(f\"Từ đồng nghĩa: {synonyms}\")\\n    for entry in info:\\n        pos_tag = entry[\\'pos_tag\\']\\n        category = entry[\\'category\\'] if entry[\\'category\\'] else \\'Không có danh mục\\'\\n        print(f\"  - POS-TAG: {pos_tag}, Category: {category}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Giả sử words_data đã được xử lý từ các file CSV\n",
        "synonyms_dict = words_data.generate_synonyms_dict()\n",
        "\n",
        "# In ra từ điển\n",
        "'''for synonyms, info in synonyms_dict.items():\n",
        "    print(f\"Từ đồng nghĩa: {synonyms}\")\n",
        "    for entry in info:\n",
        "        pos_tag = entry['pos_tag']\n",
        "        category = entry['category'] if entry['category'] else 'Không có danh mục'\n",
        "        print(f\"  - POS-TAG: {pos_tag}, Category: {category}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMtbS2aeUEhu",
        "outputId": "c1967a4f-8c2d-487a-daa0-bf0b7b9fe436"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8812"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "len(synonyms_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "zwoKA18jUEhv",
        "outputId": "6713dde9-4faa-41f1-9f2f-8591c7bb8368"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'for key, value in numbered_dict.items():\\n    synonyms = value[\\'synonyms\\']\\n    pos_tag = value[\\'info\\'][0]\\n    category = value[\\'info\\'][1] if value[\\'info\\'][1] else \\'Không có danh mục\\'\\n    print(f\"Key {key}: {synonyms}, POS-TAG: {pos_tag}, Category: {category}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Giả sử words_data đã được xử lý từ các file CSV\n",
        "numbered_dict = words_data.generate_numbered_synonyms_dict()\n",
        "\n",
        "# In ra từ điển\n",
        "'''for key, value in numbered_dict.items():\n",
        "    synonyms = value['synonyms']\n",
        "    pos_tag = value['info'][0]\n",
        "    category = value['info'][1] if value['info'][1] else 'Không có danh mục'\n",
        "    print(f\"Key {key}: {synonyms}, POS-TAG: {pos_tag}, Category: {category}\")'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpmI_jh7UEhv",
        "outputId": "7e8d0670-6cdb-48bc-f111-b76a67ec3017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9521\n"
          ]
        }
      ],
      "source": [
        "print(len(numbered_dict))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "UPwhuZZPUEhw",
        "outputId": "beaeb9e1-f7f5-4c13-a919-8f45e7191462",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys containing words'rực_rỡ': [7276, 7397, 7438, 7441, 7878, 7971, 8006, 8155, 9086, 9481]\n"
          ]
        }
      ],
      "source": [
        "# Find keys that contain the word 'brilliant' in the synonym list\n",
        "word_to_find = 'rực_rỡ'\n",
        "matching_keys = words_data.find_keys_by_word(word_to_find, numbered_dict)\n",
        "\n",
        "print(f\"Keys containing words'{word_to_find}': {matching_keys}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in matching_keys:\n",
        "  print (numbered_dict[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJAEcafiV2vP",
        "outputId": "887882ee-5a6f-496d-db75-fd474772efcb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'synonyms': ['rực_rỡ', 'đa_sắc'], 'info': ['adj', None]}\n",
            "{'synonyms': ['nguy_nga', 'tráng_lệ', 'lộng_lẫy', 'huy_hoàng', 'rực_rỡ', 'hoa_lệ'], 'info': ['adj', None]}\n",
            "{'synonyms': ['rực_rỡ', 'chói_lọi', 'sặc_sỡ', 'loè_loẹt'], 'info': ['adj', None]}\n",
            "{'synonyms': ['rực', 'chói_lọi', 'sặc_sỡ', 'rực_rỡ', 'loè_loẹt', 'màu_mè'], 'info': ['adj', None]}\n",
            "{'synonyms': ['sáng_chói', 'sáng_ngời', 'rực_rỡ', 'huy_hoàng', 'chói_lọi'], 'info': ['adj', None]}\n",
            "{'synonyms': ['sáng', 'rực_rỡ', 'sáng_chói', 'sáng_ngời', 'chói_lọi', 'huy_hoàng', 'lộng_lẫy'], 'info': ['adj', None]}\n",
            "{'synonyms': ['lộng_lẫy', 'rực_rỡ'], 'info': ['adj', None]}\n",
            "{'synonyms': ['sáng_rực', 'rực_rỡ'], 'info': ['adj', None]}\n",
            "{'synonyms': ['rực_rỡ', 'huy_hoàng', 'tráng_lệ', 'hoa_lệ', 'lộng_lẫy'], 'info': ['adj', None]}\n",
            "{'synonyms': ['rực_rỡ'], 'info': ['adv', None]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "VQPAVmRRUEhw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def load_phobert_model():\n",
        "   # Load the tokenizer and the PhoBERT model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)\n",
        "    model = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "    return tokenizer, model\n",
        "\n",
        "# Function to calculate the embedding of a sentence\n",
        "def get_sentence_embedding(sentence, tokenizer, model):\n",
        "    try:\n",
        "        inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        if outputs.last_hidden_state.size(1) == 0:\n",
        "            print(f\"Warning: No tokens produced for the sentence: '{sentence}'\")\n",
        "            return None\n",
        "\n",
        "        # Get the embedding of the [CLS] token to represent the embedding of the whole sentence\n",
        "        cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "        return cls_embedding\n",
        "\n",
        "    except IndexError as e:\n",
        "        print(f\"IndexError encountered: {e} - Skipping this sentence: '{sentence}'\")\n",
        "        return None  # Hoặc bạn có thể trả về một embedding mặc định như np.zeros((768,)) nếu cần\n",
        "\n",
        "\n",
        "# Function to calculate cosine similarity between two embeddings\n",
        "def cosine_similarity_score(embedding1, embedding2):\n",
        "    return cosine_similarity(embedding1, embedding2)[0][0]\n",
        "\n",
        "def find_best_key_by_context(sentence, index_to_replace, word_to_replace, keys, numbered_synonyms_dict, tokenizer, model):\n",
        "    if len(keys) == 1:\n",
        "        return keys[0]\n",
        "    else:\n",
        "        original_sentence = \" \".join(sentence)\n",
        "        original_embedding = get_sentence_embedding(original_sentence, tokenizer, model)\n",
        "\n",
        "        if original_embedding is None:\n",
        "            return None\n",
        "\n",
        "        best_key = None\n",
        "        best_similarity = -1\n",
        "\n",
        "        for key in keys:\n",
        "            synonyms = numbered_synonyms_dict[key]['synonyms']\n",
        "\n",
        "            for synonym in synonyms:\n",
        "                if synonym == word_to_replace:\n",
        "                    continue\n",
        "\n",
        "                replaced_sentence = sentence.copy()\n",
        "                replaced_sentence[index_to_replace] = synonym\n",
        "                replaced_sentence_str = \" \".join(replaced_sentence)\n",
        "\n",
        "                replaced_embedding = get_sentence_embedding(replaced_sentence_str, tokenizer, model)\n",
        "\n",
        "                if replaced_embedding is None:\n",
        "                    continue\n",
        "\n",
        "                similarity = cosine_similarity_score(original_embedding, replaced_embedding)\n",
        "\n",
        "                if similarity > best_similarity:\n",
        "                    best_similarity = similarity\n",
        "                    best_key = key\n",
        "\n",
        "        return best_key\n",
        "\n",
        "tokenizer, model = load_phobert_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Giả sử chúng ta có câu input và các key tương ứng chứa từ đồng nghĩa\n",
        "#sentence_1 = [\"ánh\", \"mặt_trời\", \"sáng\", \"rực_rỡ\", \"xua\", \"tan\", \"đi\", \"làn_mây\", \"đen\", \"tối\"]\n",
        "sentence = [\"ánh\", \"lửa\", \"bập_bùng\", \"cháy\", \"rực_rỡ\", \"trong\",\"buổi\", \"đốt\",\"lửa\",\"trại\", \"của\", \"đêm\", \"giao_thừa\"]\n",
        "index_to_replace = 4  # Vị trí của từ 'rực_rỡ'\n",
        "word_to_replace = sentence[index_to_replace]\n",
        "\n",
        "# Danh sách các key chứa từ đồng nghĩa với 'rực_rỡ' (giả sử đã tìm được)\n",
        "keys = [1, 2, 3 , 4]\n",
        "\n",
        "# Từ điển numbered_synonyms_dict giả định\n",
        "numbered_synonyms_dict = {\n",
        "\n",
        "    1: {'synonyms': ['rực_rỡ', 'huy_hoàng', 'tráng_lệ', 'hoa_lệ', 'lộng_lẫy', 'nguy_nga' ], 'info': ['adj', None]},\n",
        "    2: {'synonyms': ['rực_rỡ', 'chói_lọi', 'sặc_sỡ', 'loè_loẹt'], 'info': ['adj', None]},\n",
        "    3: {'synonyms': ['sáng_chói', 'sáng_ngời', 'rực_rỡ', 'huy_hoàng', 'chói_lọi'], 'info': ['adj', None]},\n",
        "    4: {'synonyms': ['rực_rỡ', 'đa_sắc' ], 'info': ['adj', None]}\n",
        "}\n",
        "\n",
        "# Tải mô hình PhoBERT\n",
        "\n",
        "\n",
        "# Tìm key phù hợp nhất dựa trên ngữ cảnh\n",
        "best_key , best_similarity = find_best_key_by_context(sentence, index_to_replace, word_to_replace, keys, numbered_synonyms_dict, tokenizer, model)\n",
        "\n",
        "# In ra kết quả\n",
        "print(f\"Key phù hợp nhất: {best_key}  best_similarity {best_similarity}\")"
      ],
      "metadata": {
        "id": "cOoI_v4sSVpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA5HTmtZUEhx",
        "outputId": "33c8b643-2eb6-4bfe-cc2e-170584f02297"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key phù hợp nhất: 168 time =0.9431040287017822\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "best_key = find_best_key_by_context(sentence, index_to_replace, word_to_replace, keys, numbered_synonyms_dict, tokenizer, model)\n",
        "print(f\"Key phù hợp nhất: {best_key} time = {time.time() - start_time}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "WYAE1kJrUEhy"
      },
      "outputs": [],
      "source": [
        "def encoding_sentence(sentence):\n",
        "    result = []\n",
        "    for index, word in enumerate(sentence):\n",
        "        if word not in all_words:\n",
        "            if word.isdigit():\n",
        "              result.append(\"Num\")\n",
        "            else:\n",
        "              result.append(0)\n",
        "        else:\n",
        "            keys =  words_data.find_keys_by_word(word, numbered_dict)\n",
        "            best_key = find_best_key_by_context(sentence, index, word, keys, numbered_dict, tokenizer, model)\n",
        "            result.append(best_key)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_sentences[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyIOaSwaHX3h",
        "outputId": "f8dd90b4-8783-4256-81b8-3dac8ae547b1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bộ',\n",
              " 'y_tế',\n",
              " 'đã',\n",
              " 'có',\n",
              " 'quyết_định',\n",
              " '2609',\n",
              " 'qđ-byt',\n",
              " 'ban_hành',\n",
              " 'hướng_dẫn',\n",
              " 'phòng',\n",
              " 'và',\n",
              " 'kiểm_soát',\n",
              " 'lây_nhiễm',\n",
              " 'covid-19',\n",
              " 'trong',\n",
              " 'cơ_sở',\n",
              " 'khám',\n",
              " 'bệnh',\n",
              " 'chữa',\n",
              " 'bệnh']"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "oCipnzyzUEhy",
        "outputId": "28e2deb7-8936-4389-fda4-5ff8b9203196",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoding: [5720, 0, 7888, 2928, 4559, 'Num', 0, 1872, 1608, 1025, 9519, 941, 4082, 0, 8111, 4963, 580, 3949, 3090, 3949] time = 21.98444151878357\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "encoding = encoding_sentence(list_sentences[0])\n",
        "print(f\"encoding: {encoding} time = {time.time() - start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "EygLO_MUUEhz",
        "outputId": "ffd6c392-d8f6-4fc0-b7af-cd4820f314ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words encoded as 0: 492\n"
          ]
        }
      ],
      "source": [
        "encoding_res = []\n",
        "count_zero = 0\n",
        "for i in range (len(list_sentences)):\n",
        "  encoding = encoding_sentence(list_sentences[0])\n",
        "  count_zero += encoding.count(0)\n",
        "  encoding_res.append(encoding)\n",
        "\n",
        "print(f\"Total words encoded as 0: {count_zero}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f5JZS8c6KaxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}